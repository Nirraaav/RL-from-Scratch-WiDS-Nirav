{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment : Week 2\n",
    "## Finding best policies in simple MDPs\n",
    "\n",
    "Great work making the MDPs in Week 1!\n",
    "\n",
    "In this assignment, we'll use the simplest RL techniques - Policy and Value iteration to find the best policies (which maximize the discounted total reward) in our MDPs from last week.\n",
    "\n",
    "Feel free to use your own MDPs, or import them from the OpenAI Gym library.\n",
    "\n",
    "You can start this assignment during/after reading Grokking Ch-3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us recall the equation to find the value function of agent's states under a policy $\\pi$ -\n",
    "$$v_{\\pi}(s) = \\sum _{a} \\pi(a|s) ~ \\left( ~ \\sum _{s', r} ~ p(s', r | s, a) ~ \\left[r + \\gamma v_{\\pi}(s') \\right] ~ \\right)$$\n",
    "\n",
    "We can observe that the value function $v_{\\pi}$ has a lot of circular dependencies on different states. \n",
    "\n",
    "To solve such equations, one of the ways is to iteratively calculate the RHS and replace the LHS by it until the $v_{\\pi}(s)$ values start to converge. \n",
    "\n",
    "The point of convergence makes all the equations simultaneously true and hence is the required solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us calculate the value functions for some policies in the MDPs we created last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 0 - Bandit Walk\n",
    "\n",
    "Again, we consider the BW environment on Page 39.\n",
    "\n",
    "Let's consider what seems to be the most natural policy - always go Right.\n",
    "\n",
    "This environment is so simple, that we can simply calculate the value functions by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that by convention for the terminal states, \n",
    "$$v_{\\pi}(0) = v_{\\pi}(2) = 0$$\n",
    "\n",
    "Now, \n",
    "$$v_{\\pi}(1) = 1 + \\gamma \\cdot v_{\\pi}(2) = 1$$\n",
    "\n",
    "Note both the summations just have one term due to the deterministic nature of the environment and the policy (check which summation was corresponding to which stochastic variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment 1 - Slippery Walk\n",
    "\n",
    "Let's now try to solve the SWF environment from Page 67 for the naturally adversarial policy - always go Left.\n",
    "\n",
    "Since we have 5 coupled equations for states 1-5 with 5 unknown variables, we'll use Python to bruteforce the solution.\n",
    "\n",
    "To align with Grokking, let us consider an unusual $\\gamma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 0 is to import stuff\n",
    "\n",
    "import gym, gym_walk\n",
    "import numpy as np\n",
    "from gym.envs.toy_text.frozen_lake import generate_random_map\n",
    "import pprint\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0.5000000000000001, 0, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, True),\n",
      "         (0.16666666666666666, 0, 0.0, True)],\n",
      "     1: [(0.5000000000000001, 0, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, True),\n",
      "         (0.16666666666666666, 0, 0.0, True)]},\n",
      " 1: {0: [(0.5000000000000001, 0, 0.0, True),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.16666666666666666, 2, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.16666666666666666, 0, 0.0, True)]},\n",
      " 2: {0: [(0.5000000000000001, 1, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.16666666666666666, 3, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.16666666666666666, 1, 0.0, False)]},\n",
      " 3: {0: [(0.5000000000000001, 2, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.16666666666666666, 4, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 4, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.16666666666666666, 2, 0.0, False)]},\n",
      " 4: {0: [(0.5000000000000001, 3, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.16666666666666666, 5, 0.0, False)],\n",
      "     1: [(0.5000000000000001, 5, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.16666666666666666, 3, 0.0, False)]},\n",
      " 5: {0: [(0.5000000000000001, 4, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, False),\n",
      "         (0.16666666666666666, 6, 1.0, True)],\n",
      "     1: [(0.5000000000000001, 6, 1.0, True),\n",
      "         (0.3333333333333333, 5, 0.0, False),\n",
      "         (0.16666666666666666, 4, 0.0, False)]},\n",
      " 6: {0: [(0.5000000000000001, 6, 0.0, True),\n",
      "         (0.3333333333333333, 6, 0.0, True),\n",
      "         (0.16666666666666666, 6, 0.0, True)],\n",
      "     1: [(0.5000000000000001, 6, 0.0, True),\n",
      "         (0.3333333333333333, 6, 0.0, True),\n",
      "         (0.16666666666666666, 6, 0.0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "# Step 1 is to get the MDP\n",
    "\n",
    "env = gym.make('SlipperyWalkFive-v0')\n",
    "swf_mdp = env.P\n",
    "pprint.pprint(swf_mdp)\n",
    "\n",
    "# Note that in Gym, action \"Left\" is \"0\" and \"Right\" is \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 is to write the policy\n",
    "\n",
    "pi = {\n",
    "    0 : 0,\n",
    "    1 : 0,\n",
    "    2 : 0,\n",
    "    3 : 0,\n",
    "    4 : 0,\n",
    "    5 : 0,\n",
    "    6 : 0\n",
    "}\n",
    "\n",
    "# Or you can do it randomly\n",
    "# pi = dict()\n",
    "# for state in mdp:\n",
    "#     pi[state] = np.random.choice(mdp[state].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0, 1: 0, 2: 0, 3: 0, 4: 0, 5: 0, 6: 0}\n"
     ]
    }
   ],
   "source": [
    "# Step 3 is computing the value function for this envi and policy\n",
    "\n",
    "# Let us start with a random value function\n",
    "\n",
    "val = dict()\n",
    "for state in swf_mdp:\n",
    "    # val[state] = np.random.random()\n",
    "    val[state] = 0\n",
    "\n",
    "# Since 0 and 6 are terminal states, we know their values are 0\n",
    "\n",
    "val[0] = 0\n",
    "val[6] = 0\n",
    "\n",
    "pprint.pprint(val)\n",
    "\n",
    "#Or you could do it randomly, remember to set the terminal states to 0. You can also implement this while evaluating the value function using \n",
    "# val = dict()\n",
    "# for state in mdp:\n",
    "#     val[state] = np.random.random()\n",
    "#     if mdp[state][0][0][0] == 0: # if the first action in the first outcome of the first state is 0, then it is a terminal state\n",
    "#         val[state] = 0\n",
    "\n",
    "#instead of doing thsi you can simply intialize the value function to 0 for all states \n",
    "# for state in swf_mdp:\n",
    "#   val[state] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_value_fn(val, mdp, pi, gamma = 1.0):\n",
    "    \n",
    "    new_val = dict()\n",
    "    for state in mdp:\n",
    "        action = pi[state]\n",
    "        # print(action)\n",
    "        probability = mdp[state][action]\n",
    "\n",
    "        new_val_for_state = 0\n",
    "        for (transition_probability, next_state, reward, isTerminal) in probability:\n",
    "            new_val_for_state += transition_probability * (reward + gamma * val[next_state] * (not isTerminal))\n",
    "\n",
    "        new_val[state] = new_val_for_state\n",
    "    return new_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(get_new_value_fn(val, swf_mdp, pi, gamma = 1.0))\n",
    "# print(get_new_value_fn(get_new_value_fn(val, swf_mdp, pi, gamma = 1.0), swf_mdp, pi, gamma = 1.0))\n",
    "# print(get_new_value_fn(get_new_value_fn(get_new_value_fn(val, swf_mdp, pi, gamma = 1.0), swf_mdp, pi, gamma = 1.0), swf_mdp, pi, gamma = 1.0))\n",
    "# print(get_new_value_fn(get_new_value_fn(get_new_value_fn(get_new_value_fn(val, swf_mdp, pi, gamma = 1.0), swf_mdp, pi, gamma = 1.0), swf_mdp, pi, gamma = 1.0), swf_mdp, pi, gamma = 1.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use to above function to get the new value function, also print how many iterations it took to converge\n",
    "def policy_evaluation(val, mdp, pi, epsilon=1e-10, gamma=1.0):\n",
    "    count = 1\n",
    "    while True:\n",
    "        val_new = get_new_value_fn(val, mdp, pi, gamma)\n",
    "        if np.max(np.abs(np.array(list(val_new.values())) - np.array(list(val.values())))) < epsilon:\n",
    "            return val, count\n",
    "        else:\n",
    "            count += 1\n",
    "            val = val_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint.pprint(policy_evaluation(val, swf_mdp, pi, gamma = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform policy improvement using the polivy and the value function and return a new policy, the action value function should be a nested dictionary\n",
    "def policy_improvement(val, mdp, pi, gamma=1.0):\n",
    "    new_pi = dict()\n",
    "    q = dict()\n",
    "    # q must be something like\n",
    "    # {0: {0: val, 1: val}, 1: {0: val, 1: val},...}\n",
    "\n",
    "    for state in mdp:\n",
    "        q[state] = dict() # initialization, each value will be a dictionary\n",
    "        for action in mdp[state]:\n",
    "            q[state][action] = 0\n",
    "            # prob_tuples = mdp[state][action]\n",
    "            for (transition_probability, next_state, reward, isTerminal) in mdp[state][action]:\n",
    "                q[state][action] += transition_probability * (reward + gamma * val[next_state] * (not isTerminal))\n",
    "            \n",
    "    # after q has been made, \n",
    "    for state in q:\n",
    "        keys, values = np.array(list(q[state].keys())), np.array(list(q[state].values()))\n",
    "        new_pi[state] = keys[np.argmax(values)] # q[state] is a dict\n",
    "        \n",
    "    return new_pi, q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above functions to get the optimal policy and optimal value function and return the total number of iterations it took to converge\n",
    "# Create a random policy and value function to start with or use the ones defined above\n",
    "def policy_iteration(mdp, epsilon=1e-10, gamma=1.0):\n",
    "    optimal_pi = {s: 0 for s in mdp}\n",
    "    optimal_val = {s: 0 for s in mdp} # both pi and val are all zeroes initially\n",
    "    # as always, it is possible to create a random policy and value function to start with\n",
    "    count = 0\n",
    "    \n",
    "    while True:\n",
    "        new_pi = policy_improvement(optimal_val, mdp, gamma)[0]\n",
    "        if new_pi == optimal_pi: # convergence when policy cannot be optimized further\n",
    "            return optimal_pi, optimal_val, count + 1\n",
    "        else:\n",
    "            count += 1\n",
    "            optimal_pi = new_pi\n",
    "            optimal_val = policy_evaluation(optimal_val, mdp, optimal_pi, gamma = gamma)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now perform value iteration, note that the value function is a dictionary and not a list, also return the number of iterations it took to converge\n",
    "def value_iteration(mdp, gamma=1.0, epsilon=1e-10):\n",
    "    val = {s: 0 for s in mdp}\n",
    "    count = 1\n",
    "    \n",
    "    while True:\n",
    "\n",
    "        q = dict()\n",
    "\n",
    "        for state in mdp:\n",
    "            q[state] = dict()\n",
    "            for action in mdp[state]:\n",
    "                q[state][action] = 0\n",
    "                for (transition_probability, next_state, reward, isTerminal) in mdp[state][action]:\n",
    "                    q[state][action] += transition_probability * (reward + gamma * val[next_state] * (not isTerminal))\n",
    "\n",
    "        val_new = {state: max(q[state].values()) for state in mdp}\n",
    "                    \n",
    "        if np.max(np.abs(np.array(list(val_new.values())) - np.array(list(val.values())))) < epsilon:\n",
    "            break\n",
    "\n",
    "        val = val_new.copy()\n",
    "        count += 1\n",
    "\n",
    "    pi = {s: 0 for s in mdp}\n",
    "    for state in mdp:\n",
    "        keys, values = np.array(list(q[state].keys())), np.array(list(q[state].values()))\n",
    "        pi[state] = keys[np.argmax(values)]\n",
    "        \n",
    "    return val, pi, count\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enviroment 2 - Frozen Lake\n",
    "\n",
    "Repeat the above steps for the frozen lake environment. Don't create new functions , use the old functions.\n",
    "\n",
    "You can also write a function `test_policy()` to test your policy after training to find the number of times you reached the goal state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {0: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)]},\n",
      " 1: {0: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 0, 0.0, False)]},\n",
      " 2: {0: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 6, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 1, 0.0, False),\n",
      "         (0.3333333333333333, 6, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 6, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 1, 0.0, False)]},\n",
      " 3: {0: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 3, 0.0, False),\n",
      "         (0.3333333333333333, 2, 0.0, False)]},\n",
      " 4: {0: [(0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     2: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 0, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)]},\n",
      " 5: {0: [(1.0, 5, 0, True)],\n",
      "     1: [(1.0, 5, 0, True)],\n",
      "     2: [(1.0, 5, 0, True)],\n",
      "     3: [(1.0, 5, 0, True)]},\n",
      " 6: {0: [(0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 10, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True)],\n",
      "     2: [(0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 7, 0.0, True),\n",
      "         (0.3333333333333333, 2, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)]},\n",
      " 7: {0: [(1.0, 7, 0, True)],\n",
      "     1: [(1.0, 7, 0, True)],\n",
      "     2: [(1.0, 7, 0, True)],\n",
      "     3: [(1.0, 7, 0, True)]},\n",
      " 8: {0: [(0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 12, 0.0, True)],\n",
      "     1: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 12, 0.0, True),\n",
      "         (0.3333333333333333, 9, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 12, 0.0, True),\n",
      "         (0.3333333333333333, 9, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False)],\n",
      "     3: [(0.3333333333333333, 9, 0.0, False),\n",
      "         (0.3333333333333333, 4, 0.0, False),\n",
      "         (0.3333333333333333, 8, 0.0, False)]},\n",
      " 9: {0: [(0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 13, 0.0, False)],\n",
      "     1: [(0.3333333333333333, 8, 0.0, False),\n",
      "         (0.3333333333333333, 13, 0.0, False),\n",
      "         (0.3333333333333333, 10, 0.0, False)],\n",
      "     2: [(0.3333333333333333, 13, 0.0, False),\n",
      "         (0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True)],\n",
      "     3: [(0.3333333333333333, 10, 0.0, False),\n",
      "         (0.3333333333333333, 5, 0.0, True),\n",
      "         (0.3333333333333333, 8, 0.0, False)]},\n",
      " 10: {0: [(0.3333333333333333, 6, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 11, 0.0, True)],\n",
      "      2: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 11, 0.0, True),\n",
      "          (0.3333333333333333, 6, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 11, 0.0, True),\n",
      "          (0.3333333333333333, 6, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False)]},\n",
      " 11: {0: [(1.0, 11, 0, True)],\n",
      "      1: [(1.0, 11, 0, True)],\n",
      "      2: [(1.0, 11, 0, True)],\n",
      "      3: [(1.0, 11, 0, True)]},\n",
      " 12: {0: [(1.0, 12, 0, True)],\n",
      "      1: [(1.0, 12, 0, True)],\n",
      "      2: [(1.0, 12, 0, True)],\n",
      "      3: [(1.0, 12, 0, True)]},\n",
      " 13: {0: [(0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 12, 0.0, True),\n",
      "          (0.3333333333333333, 13, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 12, 0.0, True),\n",
      "          (0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      2: [(0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 9, 0.0, False),\n",
      "          (0.3333333333333333, 12, 0.0, True)]},\n",
      " 14: {0: [(0.3333333333333333, 10, 0.0, False),\n",
      "          (0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False)],\n",
      "      1: [(0.3333333333333333, 13, 0.0, False),\n",
      "          (0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 15, 1.0, True)],\n",
      "      2: [(0.3333333333333333, 14, 0.0, False),\n",
      "          (0.3333333333333333, 15, 1.0, True),\n",
      "          (0.3333333333333333, 10, 0.0, False)],\n",
      "      3: [(0.3333333333333333, 15, 1.0, True),\n",
      "          (0.3333333333333333, 10, 0.0, False),\n",
      "          (0.3333333333333333, 13, 0.0, False)]},\n",
      " 15: {0: [(1.0, 15, 0, True)],\n",
      "      1: [(1.0, 15, 0, True)],\n",
      "      2: [(1.0, 15, 0, True)],\n",
      "      3: [(1.0, 15, 0, True)]}}\n"
     ]
    }
   ],
   "source": [
    "mdp2 = gym.make('FrozenLake-v1').P\n",
    "pprint.pprint(mdp2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env2 = gym.make('FrozenLake-v1',desc=generate_random_map(size=4))\n",
    "# mdp2 = env2.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0,\n",
      " 1: 3,\n",
      " 2: 3,\n",
      " 3: 3,\n",
      " 4: 0,\n",
      " 5: 0,\n",
      " 6: 0,\n",
      " 7: 0,\n",
      " 8: 3,\n",
      " 9: 1,\n",
      " 10: 0,\n",
      " 11: 0,\n",
      " 12: 0,\n",
      " 13: 2,\n",
      " 14: 1,\n",
      " 15: 0}\n",
      "{0: 0.8235294093675635,\n",
      " 1: 0.8235294085640291,\n",
      " 2: 0.8235294079934693,\n",
      " 3: 0.8235294076974147,\n",
      " 4: 0.8235294095420496,\n",
      " 5: 0.0,\n",
      " 6: 0.5294117629918788,\n",
      " 7: 0.0,\n",
      " 8: 0.8235294098783209,\n",
      " 9: 0.8235294103519006,\n",
      " 10: 0.7647058811069277,\n",
      " 11: 0.0,\n",
      " 12: 0.0,\n",
      " 13: 0.88235294017329,\n",
      " 14: 0.9411764700677003,\n",
      " 15: 0.0}\n",
      "7\n",
      "{0: 0.8235294093738184,\n",
      " 1: 0.8235294085723809,\n",
      " 2: 0.82352940800331,\n",
      " 3: 0.8235294077080278,\n",
      " 4: 0.823529409547849,\n",
      " 5: 0.0,\n",
      " 6: 0.5294117629963512,\n",
      " 7: 0.0,\n",
      " 8: 0.823529409883243,\n",
      " 9: 0.823529410355587,\n",
      " 10: 0.764705881110179,\n",
      " 11: 0.0,\n",
      " 12: 0.0,\n",
      " 13: 0.8823529401759076,\n",
      " 14: 0.9411764700690586,\n",
      " 15: 0.0}\n",
      "{0: 0,\n",
      " 1: 3,\n",
      " 2: 3,\n",
      " 3: 3,\n",
      " 4: 0,\n",
      " 5: 0,\n",
      " 6: 0,\n",
      " 7: 0,\n",
      " 8: 3,\n",
      " 9: 1,\n",
      " 10: 0,\n",
      " 11: 0,\n",
      " 12: 0,\n",
      " 13: 2,\n",
      " 14: 1,\n",
      " 15: 0}\n",
      "806\n"
     ]
    }
   ],
   "source": [
    "pi1, val1, count1 = policy_iteration(mdp2)\n",
    "pprint.pprint(pi1)\n",
    "pprint.pprint(val1)\n",
    "pprint.pprint(count1)\n",
    "pi2, val2, count2 = value_iteration(mdp2)\n",
    "pprint.pprint(pi2)\n",
    "pprint.pprint(val2)\n",
    "pprint.pprint(count2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fl_mdp = {}\n",
    "\n",
    "## (transition probability, next state, reward, isTerminal)\n",
    "\n",
    "terminal_states = [5, 7, 11, 12, 15]\n",
    "correspondance = {\"Left\" : 0, \"Down\" : 1, \"Right\" : 2, \"Up\" : 3}\n",
    "direction = [-1, +4, +1, -4]\n",
    "\n",
    "actions_dict = {\n",
    "    0: [3, 0, 1],\n",
    "    1: [0, 1, 2],\n",
    "    2: [1, 2, 3],\n",
    "    3: [2, 3, 0]\n",
    "}\n",
    "\n",
    "\n",
    "def new_state(state, move):\n",
    "    if(move == -4 and state + move < 0):\n",
    "        return state\n",
    "    if(move == 4 and state + move > 15):\n",
    "        return state\n",
    "    if(move == 1 and (state + move) % 4 == 0):\n",
    "        return state\n",
    "    if(move == -1 and (state + move + 4) % 4 == 3):\n",
    "        return state\n",
    "    else:\n",
    "        return state + move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_policy(pi, env, goalstate = 15):\n",
    "    success = 0\n",
    "    failure = 0\n",
    "    LIMIT = 10000\n",
    "\n",
    "    for i in range(LIMIT):\n",
    "        state = 0\n",
    "        while True:\n",
    "            chance = random.random()\n",
    "            moves = actions_dict[pi[state]]\n",
    "            if chance < 1/3:\n",
    "                state = new_state(state, direction[moves[0]])\n",
    "            elif chance < 2/3:\n",
    "                state = new_state(state, direction[moves[1]])\n",
    "            else:\n",
    "                state = new_state(state, direction[moves[2]])\n",
    "\n",
    "            if state == goalstate:\n",
    "                success += 1\n",
    "                break\n",
    "\n",
    "            if state in terminal_states:\n",
    "                failure += 1\n",
    "                break\n",
    "            \n",
    "    return f\"{(success/LIMIT)*100}% times it reached goal state\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.45% times it reached goal state\n",
      "82.64% times it reached goal state\n",
      "83.25% times it reached goal state\n",
      "82.11% times it reached goal state\n",
      "82.22% times it reached goal state\n",
      "82.59% times it reached goal state\n",
      "82.28999999999999% times it reached goal state\n",
      "82.38% times it reached goal state\n",
      "81.82000000000001% times it reached goal state\n",
      "82.39999999999999% times it reached goal state\n",
      "81.93% times it reached goal state\n",
      "82.58% times it reached goal state\n",
      "82.17% times it reached goal state\n",
      "82.38% times it reached goal state\n",
      "82.25% times it reached goal state\n"
     ]
    }
   ],
   "source": [
    "for i in range(15):\n",
    "    print(test_policy(pi1, mdp2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
